{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d36636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.1\n",
      "  Downloading numpy-1.23.1-cp39-cp39-win_amd64.whl.metadata (2.2 kB)\n",
      "Downloading numpy-1.23.1-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "   ---------------------------------------- 0.0/14.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.7 MB 3.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.1/14.7 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.9/14.7 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.7/14.7 MB 6.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.5/14.7 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.1/14.7 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.4/14.7 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.5/14.7 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.1/14.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.6/14.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.7/14.7 MB 6.4 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "Successfully installed numpy-1.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.10.0 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d2c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9531029e8aff4ff4af13913257dd50d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea2c4546c7446e2a36ca6e881a81dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5014 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "HFDataset() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_len_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m---> 69\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHFDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m HFDataset(test_ds, transform\u001b[38;5;241m=\u001b[39mtest_transforms)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 5. Load base model\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: HFDataset() takes no arguments"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "from transformers import ViTImageProcessor\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Paths and Constants\n",
    "DATASET_DIR = \"dataset\"\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "OUTPUT_DIR = \"./vit-weed-pest-model-lora\"\n",
    "\n",
    "# 1. Load dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=DATASET_DIR)\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]\n",
    "\n",
    "# 2. Label mappings\n",
    "labels = train_ds.features[\"label\"].names\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "num_labels = len(labels)\n",
    "\n",
    "# 3. Image Processor & Transforms\n",
    "image_processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "# Custom Dataset wrapper to apply transforms\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]  # PIL Image\n",
    "        label = item[\"label\"]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\"pixel_values\": image, \"label\": label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Wrap datasets with transforms\n",
    "train_dataset = HFDataset(train_ds, transform=train_transforms)\n",
    "test_dataset = HFDataset(test_ds, transform=test_transforms)\n",
    "\n",
    "\n",
    "# 5. Load base model\n",
    "base_model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 6. LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 7. Accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    accuracy = metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# 8. Data collator\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# 9. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# 10. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# 11. Train\n",
    "trainer.train()\n",
    "\n",
    "# 12. Predict manually with transformed test dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"]\n",
    "        outputs = model(pixel_values=inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "y_pred = np.array(all_preds)\n",
    "y_true = np.array(all_labels)\n",
    "\n",
    "# 13. Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[id2label[i] for i in range(num_labels)])\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix on Test Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 14. Save model\n",
    "model_path = os.path.join(OUTPUT_DIR, \"final\")\n",
    "trainer.save_model(model_path)\n",
    "print(f\"✅ LoRA Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91914e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d045e9cdb9b04851bc1b94a7f5d0c397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ced7a766ca4744b12f457e1eb8dc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5014 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 86,110,486 || trainable%: 0.3425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabari\\AppData\\Local\\Temp\\ipykernel_57160\\3190831107.py:137: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "***** Running training *****\n",
      "  Num examples = 16,089\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,006\n",
      "  Number of trainable parameters = 294,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1006' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1006 05:14 < 05:14, 1.59 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.949900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4023\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_accuracy', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sabari\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:3165\u001b[0m, in \u001b[0;36mTrainer._determine_best_metric\u001b[1;34m(self, metrics, trial)\u001b[0m\n\u001b[0;32m   3164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3165\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval_accuracy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 149\u001b[0m\n\u001b[0;32m    137\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    138\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    139\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# 11. Train\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[0;32m    154\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\sabari\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sabari\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2661\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2661\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m   2663\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2667\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sabari\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:3097\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m   3096\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m-> 3097\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_best_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n\u001b[0;32m   3100\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n",
      "File \u001b[1;32mc:\\Users\\sabari\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:3167\u001b[0m, in \u001b[0;36mTrainer._determine_best_metric\u001b[1;34m(self, metrics, trial)\u001b[0m\n\u001b[0;32m   3165\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m metrics[metric_to_check]\n\u001b[0;32m   3166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m-> 3167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   3168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, which is not found in the evaluation metrics. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3169\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3170\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m   3172\u001b[0m operator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgreater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgreater_is_better \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mless\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The `metric_for_best_model` training argument is set to 'eval_accuracy', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "from transformers import ViTImageProcessor\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Paths and Constants\n",
    "DATASET_DIR = \"dataset\"\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "OUTPUT_DIR = \"./vit-weed-pest-model-lora\"\n",
    "\n",
    "# 1. Load dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=DATASET_DIR)\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]\n",
    "\n",
    "# 2. Label mappings\n",
    "labels = train_ds.features[\"label\"].names\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "num_labels = len(labels)\n",
    "\n",
    "# 3. Image Processor & Transforms\n",
    "image_processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "# 4. Dataset Wrapper\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\"pixel_values\": image, \"label\": label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_dataset = HFDataset(train_ds, transform=train_transforms)\n",
    "test_dataset = HFDataset(test_ds, transform=test_transforms)\n",
    "\n",
    "# 5. Load base model\n",
    "base_model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 6. LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# Load accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Metric computation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)\n",
    "\n",
    "# 8. Data collator\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "#9.Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    log_level=\"info\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# 10. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=image_processor,\n",
    "    \n",
    ")\n",
    "\n",
    "# 11. Train\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# Final evaluation\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"📊 Final Accuracy: {metrics['eval_accuracy']*100:.2f}%\")\n",
    "\n",
    "\n",
    "# 13. Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[id2label[i] for i in range(num_labels)])\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix on Test Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 14. Save model\n",
    "model_path = os.path.join(OUTPUT_DIR, \"final\")\n",
    "trainer.save_model(model_path)\n",
    "print(f\"✅ LoRA Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3159c4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 11.5 Evaluate after training and print results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Evaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# 11.5 Evaluate after training and print results\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(f\"\\n📊 Evaluation Results:\")\n",
    "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc510290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_runtime': 59.2231, 'test_samples_per_second': 67.93, 'test_steps_per_second': 2.128}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "print(predictions.metrics)  # ✅ Should include \"test_accuracy\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
